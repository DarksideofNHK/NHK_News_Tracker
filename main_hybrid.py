#!/usr/bin/env python3
"""
NHK RSSå·®åˆ†è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
requestsï¼ˆ6ã‚½ãƒ¼ã‚¹ï¼‰+ Seleniumï¼ˆæ±åŒ—ï¼‰
"""
import yaml
import logging
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

from scraper_hybrid import NhkRssScraperHybrid
from parser import NhkXmlParser
from storage import ArticleStorage
from visualizer import ChangeVisualizer
from gemini_analyzer import GeminiAnalyzer

def setup_logging(config: dict):
    """ãƒ­ã‚°è¨­å®š"""
    log_config = config.get('logging', {})
    log_file = Path(log_config.get('file', 'logs/nhk_tracker.log'))
    log_file.parent.mkdir(parents=True, exist_ok=True)

    logging.basicConfig(
        level=getattr(logging, log_config.get('level', 'INFO')),
        format=log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def load_config(config_path: str = 'config.yaml') -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("="*60)
    print("NHK RSSå·®åˆ†è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ")
    print("requestsï¼ˆ6ã‚½ãƒ¼ã‚¹ï¼‰+ Seleniumï¼ˆæ±åŒ—ï¼‰")
    print("="*60)
    print(f"å®Ÿè¡Œé–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    setup_logging(config)
    logger = logging.getLogger(__name__)

    # åˆæœŸåŒ–
    scraper = NhkRssScraperHybrid()
    parser = NhkXmlParser()

    # Gemini Analyzerï¼ˆç’°å¢ƒå¤‰æ•°GEMINI_API_KEYãŒå¿…è¦ï¼‰
    gemini = GeminiAnalyzer()

    storage = ArticleStorage(db_path=config['database']['path'], gemini_analyzer=gemini)
    visualizer = ChangeVisualizer()

    # çµ±è¨ˆ
    total_stats = {'new': 0, 'updated': 0, 'unchanged': 0}

    # URLãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸€æ‹¬å–å¾—ç”¨ï¼‰
    sources_dict = {}
    for source_config in config['sources']:
        if source_config.get('enabled', True):
            sources_dict[source_config['name']] = source_config['url']

    print(f"{'â”€'*60}")
    print(f"ä¸€æ‹¬å–å¾—é–‹å§‹: {len(sources_dict)}ã‚½ãƒ¼ã‚¹")
    print(f"{'â”€'*60}\n")

    # ä¸€æ‹¬å–å¾—ï¼ˆrequests + Seleniumè‡ªå‹•åˆ‡ã‚Šæ›¿ãˆï¼‰
    contents = scraper.fetch_batch(sources_dict)

    # å„ã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†
    for source_config in config['sources']:
        if not source_config.get('enabled', True):
            continue

        name = source_config['name']
        xml_content = contents.get(name)

        print(f"\n{'â”€'*60}")
        print(f"å‡¦ç†ä¸­: {name}")
        print(f"{'â”€'*60}")

        if xml_content is None:
            print(f"âŒ å–å¾—å¤±æ•—: {name}")
            continue

        # è§£æ
        articles = parser.parse(xml_content)
        if not articles:
            print(f"âŒ è§£æå¤±æ•—: {name}")
            continue

        print(f"âœ… è¨˜äº‹å–å¾—: {len(articles)}ä»¶")

        # ä¿å­˜ã¨å¤‰æ›´æ¤œå‡º
        stats = storage.save_articles(name, articles)

        print(f"ğŸ“Š çµæœ:")
        print(f"  - æ–°è¦: {stats['new']}ä»¶")
        print(f"  - æ›´æ–°: {stats['updated']}ä»¶")
        print(f"  - å¤‰æ›´ãªã—: {stats['unchanged']}ä»¶")

        # çµ±è¨ˆé›†è¨ˆ
        for key in total_stats:
            total_stats[key] += stats[key]

    # å…¨ä½“ã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print("å…¨ä½“ã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    print(f"æ–°è¦è¨˜äº‹: {total_stats['new']}ä»¶")
    print(f"æ›´æ–°è¨˜äº‹: {total_stats['updated']}ä»¶")
    print(f"å¤‰æ›´ãªã—: {total_stats['unchanged']}ä»¶")

    # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    hours = config['report'].get('hours', 24)
    changes = storage.get_recent_changes(hours=hours)

    if changes:
        output_dir = Path(config['report']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = output_dir / f'changes_{timestamp}.html'

        visualizer.generate_html_report(changes, str(output_path), hours=hours)

        print(f"âœ… HTMLãƒ¬ãƒãƒ¼ãƒˆ: {output_path.absolute()}")
        print(f"ğŸ“Š å¤‰æ›´ä»¶æ•°: {len(changes)}ä»¶ï¼ˆéå»{hours}æ™‚é–“ï¼‰")
    else:
        print(f"â„¹ï¸  éå»{hours}æ™‚é–“ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    # JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print(f"\n{'â”€'*60}")
    print("ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    print(f"{'â”€'*60}")

    export_path = f"data/export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    storage.export_to_json(export_path)
    print(f"âœ… JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {Path(export_path).absolute()}")

    # å…¨å¤‰æ›´å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("å…¨å¤‰æ›´å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    try:
        import generate_history
        generate_history.main()
    except Exception as e:
        print(f"âš ï¸ å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.warning(f"å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆå¤±æ•—: {e}")

    # è¨˜äº‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("è¨˜äº‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    try:
        import generate_archive
        generate_archive.main()
    except Exception as e:
        print(f"âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.warning(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆå¤±æ•—: {e}")

    print(f"\n{'='*60}")
    print(f"å®Ÿè¡Œå®Œäº†: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")

if __name__ == '__main__':
    main()
