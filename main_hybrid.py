#!/usr/bin/env python3
"""
NHK RSSå·®åˆ†è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
requestsï¼ˆ6ã‚½ãƒ¼ã‚¹ï¼‰+ Seleniumï¼ˆæ±åŒ—ï¼‰
"""
import yaml
import logging
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

from scraper_hybrid import NhkRssScraperHybrid
from parser import NhkXmlParser
from storage import ArticleStorage
from visualizer import ChangeVisualizer
from gemini_analyzer import GeminiAnalyzer
from notifier import MacNotifier

def setup_logging(config: dict):
    """ãƒ­ã‚°è¨­å®š"""
    log_config = config.get('logging', {})
    log_file = Path(log_config.get('file', 'logs/nhk_tracker.log'))
    log_file.parent.mkdir(parents=True, exist_ok=True)

    logging.basicConfig(
        level=getattr(logging, log_config.get('level', 'INFO')),
        format=log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s'),
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

def load_config(config_path: str = 'config.yaml') -> dict:
    """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("="*60)
    print("NHK RSSå·®åˆ†è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ")
    print("requestsï¼ˆ6ã‚½ãƒ¼ã‚¹ï¼‰+ Seleniumï¼ˆæ±åŒ—ï¼‰")
    print("="*60)
    print(f"å®Ÿè¡Œé–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = load_config()
    setup_logging(config)
    logger = logging.getLogger(__name__)

    # åˆæœŸåŒ–
    scraper = NhkRssScraperHybrid()
    parser = NhkXmlParser()

    # Gemini Analyzerï¼ˆç’°å¢ƒå¤‰æ•°GEMINI_API_KEYãŒå¿…è¦ï¼‰
    gemini = GeminiAnalyzer()

    storage = ArticleStorage(db_path=config['database']['path'], gemini_analyzer=gemini)
    visualizer = ChangeVisualizer()

    # çµ±è¨ˆ
    total_stats = {'new': 0, 'updated': 0, 'unchanged': 0}
    failed_sources = []  # å¤±æ•—ã—ãŸã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆ
    all_correction_added = []  # è¨‚æ­£è¿½åŠ ã®ãƒªã‚¹ãƒˆ [(source, title, keywords), ...]
    all_correction_removed = []  # è¨‚æ­£å‰Šé™¤ã®ãƒªã‚¹ãƒˆ [(source, title, keywords), ...]

    # URLãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆä¸€æ‹¬å–å¾—ç”¨ï¼‰
    sources_dict = {}
    for source_config in config['sources']:
        if source_config.get('enabled', True):
            sources_dict[source_config['name']] = source_config['url']

    print(f"{'â”€'*60}")
    print(f"ä¸€æ‹¬å–å¾—é–‹å§‹: {len(sources_dict)}ã‚½ãƒ¼ã‚¹")
    print(f"{'â”€'*60}\n")

    # ä¸€æ‹¬å–å¾—ï¼ˆrequests + Seleniumè‡ªå‹•åˆ‡ã‚Šæ›¿ãˆï¼‰
    contents = scraper.fetch_batch(sources_dict)

    # NHK ONEæ¤œç´¢ï¼ˆæ±åŒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—å¾Œã«å®Ÿè¡Œï¼‰
    print(f"\n{'â”€'*60}")
    print("NHK ONEæ¤œç´¢: è¨‚æ­£è¨˜äº‹ã‚’æ¤œç´¢ä¸­...")
    print(f"{'â”€'*60}")
    nhk_one_articles = []
    try:
        nhk_one_articles = scraper.search_nhk_one(query="å¤±ç¤¼ã—ã¾ã—ãŸ")
        if nhk_one_articles:
            print(f"âœ… NHK ONEæ¤œç´¢: {len(nhk_one_articles)}ä»¶ã®è¨‚æ­£è¨˜äº‹ã‚’ç™ºè¦‹")
        else:
            print(f"â„¹ï¸  NHK ONEæ¤œç´¢: è¨‚æ­£è¨˜äº‹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    except Exception as e:
        logger.error(f"NHK ONEæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"âš ï¸ NHK ONEæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")

    # å„ã‚½ãƒ¼ã‚¹ã‚’å‡¦ç†
    for source_config in config['sources']:
        if not source_config.get('enabled', True):
            continue

        name = source_config['name']
        xml_content = contents.get(name)

        print(f"\n{'â”€'*60}")
        print(f"å‡¦ç†ä¸­: {name}")
        print(f"{'â”€'*60}")

        if xml_content is None:
            print(f"âŒ å–å¾—å¤±æ•—: {name}")
            failed_sources.append(name)
            continue

        # è§£æ
        articles = parser.parse(xml_content)
        if not articles:
            print(f"âŒ è§£æå¤±æ•—: {name}")
            continue

        print(f"âœ… è¨˜äº‹å–å¾—: {len(articles)}ä»¶")

        # ä¿å­˜ã¨å¤‰æ›´æ¤œå‡º
        stats = storage.save_articles(name, articles)

        print(f"ğŸ“Š çµæœ:")
        print(f"  - æ–°è¦: {stats['new']}ä»¶")
        print(f"  - æ›´æ–°: {stats['updated']}ä»¶")
        print(f"  - å¤‰æ›´ãªã—: {stats['unchanged']}ä»¶")

        # è¨‚æ­£ã®è¿½åŠ ãƒ»å‰Šé™¤ã‚’åé›†
        for title, keywords in stats.get('correction_added', []):
            all_correction_added.append((name, title, keywords))
            print(f"  ğŸ”´ è¨‚æ­£è¿½åŠ : {title} [ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}]")

        for title, keywords in stats.get('correction_removed', []):
            all_correction_removed.append((name, title, keywords))
            print(f"  âš ï¸  è¨‚æ­£å‰Šé™¤: {title} [ä»¥å‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}]")

        # çµ±è¨ˆé›†è¨ˆ
        for key in ['new', 'updated', 'unchanged']:
            total_stats[key] += stats[key]

    # NHK ONEæ¤œç´¢ã®è¨˜äº‹ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    if nhk_one_articles:
        print(f"\n{'â”€'*60}")
        print("NHK ONEæ¤œç´¢çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ä¸­...")
        print(f"{'â”€'*60}")
        nhk_one_stats = storage.save_articles('NHK ONEæ¤œç´¢', nhk_one_articles)

        print(f"ğŸ“Š NHK ONEæ¤œç´¢çµæœ:")
        print(f"  - æ–°è¦: {nhk_one_stats['new']}ä»¶")
        print(f"  - æ›´æ–°: {nhk_one_stats['updated']}ä»¶")
        print(f"  - å¤‰æ›´ãªã—: {nhk_one_stats['unchanged']}ä»¶")

        # è¨‚æ­£ã®è¿½åŠ ãƒ»å‰Šé™¤ã‚’åé›†
        for title, keywords in nhk_one_stats.get('correction_added', []):
            all_correction_added.append(('NHK ONEæ¤œç´¢', title, keywords))
            print(f"  ğŸ”´ è¨‚æ­£è¿½åŠ : {title} [ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}]")

        for title, keywords in nhk_one_stats.get('correction_removed', []):
            all_correction_removed.append(('NHK ONEæ¤œç´¢', title, keywords))
            print(f"  âš ï¸  è¨‚æ­£å‰Šé™¤: {title} [ä»¥å‰ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}]")

        # çµ±è¨ˆé›†è¨ˆ
        for key in ['new', 'updated', 'unchanged']:
            total_stats[key] += nhk_one_stats[key]

    # å…¨ä½“ã‚µãƒãƒªãƒ¼
    print(f"\n{'='*60}")
    print("å…¨ä½“ã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    print(f"æ–°è¦è¨˜äº‹: {total_stats['new']}ä»¶")
    print(f"æ›´æ–°è¨˜äº‹: {total_stats['updated']}ä»¶")
    print(f"å¤‰æ›´ãªã—: {total_stats['unchanged']}ä»¶")
    if all_correction_added:
        print(f"ğŸ”´ è¨‚æ­£è¿½åŠ : {len(all_correction_added)}ä»¶")
    if all_correction_removed:
        print(f"âš ï¸  è¨‚æ­£å‰Šé™¤: {len(all_correction_removed)}ä»¶")

    # HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    hours = config['report'].get('hours', 24)
    changes = storage.get_recent_changes(hours=hours)

    if changes:
        output_dir = Path(config['report']['output_dir'])
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = output_dir / f'changes_{timestamp}.html'

        visualizer.generate_html_report(changes, str(output_path), hours=hours)

        print(f"âœ… HTMLãƒ¬ãƒãƒ¼ãƒˆ: {output_path.absolute()}")
        print(f"ğŸ“Š å¤‰æ›´ä»¶æ•°: {len(changes)}ä»¶ï¼ˆéå»{hours}æ™‚é–“ï¼‰")
    else:
        print(f"â„¹ï¸  éå»{hours}æ™‚é–“ã«å¤‰æ›´ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    # JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    print(f"\n{'â”€'*60}")
    print("ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­...")
    print(f"{'â”€'*60}")

    export_path = f"data/export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    storage.export_to_json(export_path)
    print(f"âœ… JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {Path(export_path).absolute()}")

    # å…¨å¤‰æ›´å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("å…¨å¤‰æ›´å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    try:
        import generate_history
        generate_history.main()
    except Exception as e:
        print(f"âš ï¸ å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.warning(f"å±¥æ­´ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆå¤±æ•—: {e}")

    # è¨˜äº‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("è¨˜äº‹ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    try:
        import generate_archive
        generate_archive.main()
    except Exception as e:
        print(f"âš ï¸ ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.warning(f"ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒ“ãƒ¥ãƒ¼ã‚¢ãƒ¼ç”Ÿæˆå¤±æ•—: {e}")

    # ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ç”Ÿæˆ
    print(f"\n{'â”€'*60}")
    print("ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ç”Ÿæˆä¸­...")
    print(f"{'â”€'*60}")

    try:
        import generate_portal
        generate_portal.generate_portal_html()
    except Exception as e:
        print(f"âš ï¸ ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.warning(f"ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ç”Ÿæˆå¤±æ•—: {e}")

    # è¨‚æ­£ã®è¿½åŠ ãƒ»å‰Šé™¤ã‚’é€šçŸ¥
    for source, title, keywords in all_correction_added:
        MacNotifier.notify_correction_added(source, title, keywords)

    for source, title, keywords in all_correction_removed:
        MacNotifier.notify_correction_removed(source, title, keywords)

    # å®Ÿè¡Œå®Œäº†é€šçŸ¥
    total_count = sum(total_stats.values())
    MacNotifier.notify_completion(
        new_count=total_stats['new'],
        updated_count=total_stats['updated'],
        total_count=total_count,
        failed_sources=failed_sources if failed_sources else None
    )

    print(f"\n{'='*60}")
    print(f"å®Ÿè¡Œå®Œäº†: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")

if __name__ == '__main__':
    main()
